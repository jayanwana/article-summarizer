{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstract_summarizer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayanwana/article-summarizer/blob/master/abstract_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMGEGBb7qv_",
        "colab_type": "text"
      },
      "source": [
        "# Abstract Article Summarizer\n",
        "generates a summary of any article fed into it\n",
        "\n",
        "This summarizer uses tensor flow rnn to generate an abstract summary of any article fed into it.\n",
        "In order to test the model, change the test_article_path to the path of any .txt article you wish to summarize.\n",
        "this summarizer currently has a bug that causes it to crash if there are empty lines in the article you wish to summarize. And since the data is cleaned line by line, still trying to find a way around this.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlcHbtC9IrvV",
        "colab_type": "text"
      },
      "source": [
        "**Mounting Google Drive**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_WZ9Ot-NNFj",
        "colab_type": "code",
        "outputId": "7ca04bcd-44b9-4c1b-cae2-7aaa528fa02d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8XEn0JfzlRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "a02d719d-b74e-4523-c3e6-17673306cbbf"
      },
      "source": [
        "!pip install wget\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=2dda0cc1efc7f59847b62f6a544438b281007dcd4a6072641dc963f2445dcee1\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suQVQT1D0oKz",
        "colab_type": "text"
      },
      "source": [
        "**Importing** **dependencies** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f2vuOgNuNBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import time\n",
        "import re\n",
        "import wget\n",
        "import os\n",
        "import zipfile\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eXgJdj2u4We",
        "colab_type": "code",
        "outputId": "46f09b05-6b15-45bb-bb8f-1f751d285782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "nltk.download('punkt')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQrl7X_WI6TL",
        "colab_type": "text"
      },
      "source": [
        "**Creating Path Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTMcx81-RLTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#default path for the folder inside google drive\n",
        "default_path = \"/content/drive/Data/\"\n",
        "#path for training text (article)\n",
        "train_article_path = default_path + \"sumdata/train/train.article.txt\" \n",
        " #path for training text output (headline)\n",
        "train_title_path   = default_path + \"sumdata/train/train.title.txt\" \n",
        "# test article path\n",
        "test_article_path = default_path + \"test/test2.txt\"\n",
        "#path for validation text (article)\n",
        "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
        "#path for validation text output(headline)\n",
        "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ1OSCHaJxCK",
        "colab_type": "text"
      },
      "source": [
        "**Download training data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bnBWIaDJuTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(default_path):\n",
        "        os.mkdir(default_path)\n",
        "\n",
        "data_url = https://github.com/dongjun-Lee/text-summarization-tensorflow/raw/master/sample_data.zip\n",
        "# Download training Data\n",
        "wget.download(data_url, out=default_path)\n",
        "\n",
        "# Extract training Data\n",
        "with zipfile.ZipFile(os.path.join(default_path, \"sample_data.zip\"), \"r\") as z:\n",
        "    z.extractall(default_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKXXTEh8FD1",
        "colab_type": "text"
      },
      "source": [
        "Creating functions to get, clean and prepare data for Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJstVnAzuxrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_data(sentence):\n",
        "  '''\n",
        "  this function cleans each sentence from the article\n",
        "  '''\n",
        "  sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
        "  sentence = re.sub('[^\\w\\s\\n]', '', sentence)\n",
        "  sentence = re.sub('_', '', sentence)\n",
        "  sentence = re.sub('\\s+', ' ', sentence)\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg6GHTPU7oxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clean_data_list(data_path, full_data):\n",
        "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if full_data:\n",
        "            return [clean_data(line.strip()) for line in f.readlines() if len(line)>0][:200000]\n",
        "        else:\n",
        "            return [clean_data(line.strip()) for line in f.readlines() if len(line)>0][:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAey3JteMElw",
        "colab_type": "text"
      },
      "source": [
        "function that Builds dictionary (and reverse dictionary) of words and numerical keys as values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lo5qaZW9Box",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(mode, full_data=True):\n",
        "    \"\"\"\n",
        "    builds a dictionary and reverse dictionary of word keys and numeric values\n",
        "    :param mode: str; train- for training data\n",
        "                        test- for real data\n",
        "    :param full_data: Bool; full data or subset\n",
        "    :return:    word_dict, reversed_dict, article_max_len, summary_max_len\n",
        "    \"\"\"\n",
        "    if mode == \"train\":\n",
        "      # load training data\n",
        "        train_article_list = get_clean_data_list(train_article_path, full_data)\n",
        "        train_title_list = get_clean_data_list(train_title_path, full_data)\n",
        "\n",
        "      # create list of words using word tokenizer\n",
        "        words = []\n",
        "        for sentence in [*train_article_list, *train_title_list]:\n",
        "            for word in word_tokenize(sentence):\n",
        "                words.append(word)\n",
        "\n",
        "        word_counter = collections.Counter(words).most_common()\n",
        "        # create word dict\n",
        "        word_dict = {}\n",
        "        # padding for same length sequences\n",
        "        word_dict[\"<padding>\"] = 0\n",
        "        # unkown words not in dict\n",
        "        word_dict[\"<unk>\"] = 1\n",
        "        # <s> for beginning of sentence\n",
        "        word_dict[\"<s>\"] = 2\n",
        "        # end of sentence\n",
        "        word_dict[\"</s>\"] = 3\n",
        "        # create dict looping over word counter\n",
        "        for word, _ in word_counter:\n",
        "            word_dict[word] = len(word_dict)\n",
        "\n",
        "        # store word dict to be used for validation for use in test mode\n",
        "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
        "            pickle.dump(word_dict, f)\n",
        "\n",
        "    elif mode == \"test\":\n",
        "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
        "            word_dict = pickle.load(f)\n",
        "    # switcching keys and values in word dict\n",
        "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
        "\n",
        "    article_max_len = 100\n",
        "    summary_max_len = 30\n",
        "\n",
        "    return word_dict, reversed_dict, article_max_len, summary_max_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HL4iyGgMutf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V42-9BhZ9FIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(mode, word_dict, article_max_len, summary_max_len, \n",
        "                  full_data=True):\n",
        "    \"\"\"\n",
        "    builds dataset used by alogrithm\n",
        "     :param mode: str;   train- for training data\n",
        "                        test- for real data\n",
        "    :param word_dict:   dictionary of word and values \n",
        "    :param article_max_len: int; length of article\n",
        "    :param summary_max_len: int; length of summary\n",
        "    :param full_data:   Bool; full data or subset\n",
        "    :return:    words; list of words \n",
        "  \"\"\"\n",
        "    if mode == \"train\":\n",
        "        article_list = get_clean_data_list(train_article_path, full_data)\n",
        "        title_list = get_clean_data_list(train_title_path, full_data)\n",
        "    elif mode == \"test\":\n",
        "        article_list = get_clean_data_list(test_article_path, full_data)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    # words list for words in article\n",
        "    words = [word_tokenize(sentence) for sentence in article_list]\n",
        "    # if word isn't in dict use unkown token\n",
        "    words = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in word] for word in words]\n",
        "    # if words in article are less than article max length\n",
        "    words = [word[:article_max_len] for word in words]\n",
        "    # pad words using padding token\n",
        "    words = [word + (article_max_len - len(word)) * [word_dict[\"<padding>\"]] for word in words]\n",
        "\n",
        "    if mode == \"test\":\n",
        "        return words\n",
        "    else:\n",
        "        title_word = [word_tokenize(word) for word in title_list]\n",
        "        title_word = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in title_word]\n",
        "        title_word = [word[:(summary_max_len - 1)] for word in title_word]\n",
        "        return words, title_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRf8qHrn9Fb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    \"\"\"\n",
        "    \n",
        "    :param inputs: \n",
        "    :param outputs: \n",
        "    :param batch_size: \n",
        "    :param num_epochs: \n",
        "    :return: \n",
        "    \"\"\"\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVX9y38V9FnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_embedding(glove, reversed_dict, embedding_size):\n",
        "    \"\"\"\n",
        "    Returns a numpy array of word vector embedding\n",
        "    :param reversed_dict: dict();\n",
        "    :param embedding_size: int;\n",
        "    :return: np.array(word_vec_list)\n",
        "    \"\"\"\n",
        "    glove_dir = default_path + 'glove/'\n",
        "    glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
        "\n",
        "    if not os.path.exists(glove_dir):\n",
        "        os.mkdir(glove_dir)\n",
        "\n",
        "    glove_file = glove_dir + \"glove.42B.300d.txt\"\n",
        "\n",
        "    if not os.path.isfile(glove_file):\n",
        "        # Download glove vector\n",
        "        wget.download(glove_url, out=glove_dir)\n",
        "\n",
        "        # Extract glove file\n",
        "        with zipfile.ZipFile(os.path.join(glove_dir, \"glove.42B.300d.zip\"), \"r\") as z:\n",
        "            z.extractall(glove_dir)\n",
        "\n",
        "    \n",
        "    word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
        "    glove2word2vec(glove_file, word2vec_file)\n",
        "    print(\"Loading Glove vectors...\")\n",
        "    word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
        "\n",
        "    word_vec_list = []\n",
        "    for _, word in sorted(reversed_dict.items()):\n",
        "        try:\n",
        "            word_vec = word_vectors.word_vec(word)\n",
        "        except KeyError:\n",
        "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
        "\n",
        "        word_vec_list.append(word_vec)\n",
        "\n",
        "    # Assign random vector to <s>, </s> token\n",
        "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "\n",
        "    return np.array(word_vec_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whwWOpq79FCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args,forward_only=False):\n",
        "        # Innitialization Block\n",
        "        self.vocabulary_size = len(reversed_dict)\n",
        "        self.embedding_size = args.embedding_size\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.num_layers = args.num_layers\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.beam_width = args.beam_width\n",
        "        if not forward_only:\n",
        "          self.keep_prob = args.keep_prob\n",
        "        else:\n",
        "          self.keep_prob = 1.0\n",
        "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
        "        with tf.variable_scope(\"decoder/projection\"):\n",
        "          self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
        "\n",
        "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
        "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
        "        self.X_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        # Embedding Block\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "          # Check if training\n",
        "            if not forward_only and args.glove:\n",
        "              init_embeddings = tf.constant(get_init_embedding(args.glove, reversed_dict, self.embedding_size), dtype=tf.float32)\n",
        "          # Else testing\n",
        "            else:\n",
        "              init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
        "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
        "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
        "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
        "\n",
        "        # Encoding Block\n",
        "        with tf.name_scope(\"encoder\"):\n",
        "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
        "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
        "\n",
        "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
        "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
        "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
        "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
        "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
        "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
        "\n",
        "        # Decoding Block\n",
        "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
        "            decoder_cell = self.cell(self.num_hidden * 2)\n",
        "\n",
        "            if not forward_only:\n",
        "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                    attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
        "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
        "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
        "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
        "                self.decoder_output = outputs.rnn_output\n",
        "                self.logits = tf.transpose(\n",
        "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
        "                self.logits_reshape = tf.concat(\n",
        "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
        "            else:\n",
        "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
        "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
        "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                    attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
        "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                    cell=decoder_cell,\n",
        "                    embedding=self.embeddings,\n",
        "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
        "                    end_token=tf.constant(3),\n",
        "                    initial_state=initial_state,\n",
        "                    beam_width=self.beam_width,\n",
        "                    output_layer=self.projection_layer\n",
        "                )\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
        "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
        "\n",
        "        # Loss Block\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            if not forward_only:\n",
        "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
        "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
        "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
        "\n",
        "                params = tf.trainable_variables()\n",
        "                gradients = tf.gradients(self.loss, params)\n",
        "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSgYKcQ45Tyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model():\n",
        "  class Args:\n",
        "    num_hidden=150\n",
        "    num_layers=2\n",
        "    beam_width=10\n",
        "    glove=True\n",
        "    embedding_size=300\n",
        "\n",
        "    learning_rate=1e-2\n",
        "    batch_size=64\n",
        "    num_epochs=10\n",
        "    keep_prob = 0.8\n",
        "\n",
        "    full_data=True \n",
        "\n",
        "    with_model=True\n",
        "  args = Args()\n",
        "  if not os.path.exists(default_path + \"saved_model\"):\n",
        "    os.mkdir(default_path + \"saved_model\")\n",
        "\n",
        "  print(\"Building dictionary...\")\n",
        "  word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", full_data)\n",
        "  print(\"Loading training dataset...\")\n",
        "  train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, full_data)\n",
        "  print('Done...')\n",
        "  start = time.perf_counter()\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    if 'old_model_checkpoint_path' in globals():\n",
        "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
        "        saver.restore(sess, old_model_checkpoint_path )\n",
        "\n",
        "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
        "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
        "\n",
        "    print(\"\\nIteration starts.\")\n",
        "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
        "    for batch_x, batch_y in batches:\n",
        "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
        "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
        "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
        "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
        "\n",
        "        batch_decoder_input = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
        "        batch_decoder_output = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
        "\n",
        "        train_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "            model.decoder_input: batch_decoder_input,\n",
        "            model.decoder_len: batch_decoder_len,\n",
        "            model.decoder_target: batch_decoder_output\n",
        "        }\n",
        "\n",
        "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "\n",
        "        if step % num_batches_per_epoch == 0:\n",
        "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
        "            minutes, seconds = divmod(rem, 60)\n",
        "            saver.save(sess, default_path + \"saved_model/model.ckpt\", global_step=step)\n",
        "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
        "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXRacUw-FIlX",
        "colab_type": "code",
        "outputId": "fe5e45ab-1cb5-4ba6-c430-50cd964aa805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-3631ac83795f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"saved_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"saved_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building dictionary...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Data/saved_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xvPVB4ygXKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model():\n",
        "    class Args:\n",
        "        num_hidden=150\n",
        "        num_layers=2\n",
        "        beam_width=10\n",
        "        glove=True\n",
        "        embedding_size=300\n",
        "\n",
        "        learning_rate=1e-2\n",
        "        batch_size=64\n",
        "        num_epochs=10\n",
        "        keep_prob = 0.8\n",
        "\n",
        "        full_data=True\n",
        "\n",
        "        with_model=True\n",
        "\n",
        "\n",
        "    args = Args()\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    print(\"Loading dictionary...\")\n",
        "    word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"test\", args.full_data)\n",
        "    print(\"Loading test dataset...\")\n",
        "    valid_x = build_dataset(\"test\", word_dict, article_max_len, summary_max_len, args.full_data)\n",
        "    valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Loading saved model...\")\n",
        "        model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "        batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
        "\n",
        "        print(\"Writing summaries to 'result.txt'...\")\n",
        "        for batch_x, _ in batches:\n",
        "            batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
        "\n",
        "            valid_feed_dict = {\n",
        "                model.batch_size: len(batch_x),\n",
        "                model.X: batch_x,\n",
        "                model.X_len: batch_x_len,\n",
        "            }\n",
        "\n",
        "            prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
        "            prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
        "            summary_array = []\n",
        "            with open(default_path + \"result.txt\", \"a\") as f:\n",
        "                for line in prediction_output:\n",
        "                    summary = list()\n",
        "                    for word in line:\n",
        "                        if word == \"</s>\":\n",
        "                            break\n",
        "                        if word not in summary:\n",
        "                            summary.append(word)\n",
        "                    summary_array.append(\" \".join(summary))\n",
        "                    print(\" \".join(summary), file=f)\n",
        "\n",
        "        print('Summaries have been generated')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvtrgCHwmHhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "848d96e2-67e2-40c8-ebf0-00c02d08ceeb"
      },
      "source": [
        "test_model()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dictionary...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-d08da2596b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-2afc6ac452fc>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dictionary...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_max_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading test dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mvalid_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-57474becba81>\u001b[0m in \u001b[0;36mbuild_dict\u001b[0;34m(mode, full_data)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"word_dict.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mword_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# switcching keys and values in word dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Data/word_dict.pickle'"
          ]
        }
      ]
    }
  ]
}